
       MATCH (r:Repository)-[:DEPENDS_ON]->(dep:Repository)
       WHERE dep.name = $dependency
       RETURN r.full_name, r.name, r.stars, ...
       ORDER BY r.stars DESC
   
   EXAMPLE:
       find_repos_depending_on("pydantic") returns:
       - langchain (75000 stars)
       - fastapi (68000 stars)
       - ...

WHY GRAPH DATABASE:
    SQL equivalent would be:
        SELECT r.* FROM repos r
        JOIN dependencies d ON r.id = d.repo_id
        JOIN repos dep ON d.dependency_id = dep.id
        WHERE dep.name = 'pydantic'
    
    This is O(n) for each JOIN!
    
    Neo4j traversal is O(1) per hop - just follow the edge.

INTERVIEW TIP:
    "Graph databases excel at relationship queries. Finding 'repos that depend 
    on X' is a single edge traversal in Neo4j versus expensive JOINs in SQL. 
    For dependency analysis, graph is the natural data model."


--------------------------------------------------------------------------------
FILE: agent/search.py
LOCATION: agent/search.py
PURPOSE: Hybrid search combining vector + graph
--------------------------------------------------------------------------------

MAIN FUNCTION: search_repos(query, top_k=5)

STEP 1: Check Cache
    cache_key = f"{query}_{top_k}"
    if cache_key in _cache:
        return cached_result
    
    WHY: Avoid rate limits and speed up repeated queries

STEP 2: Intent Detection
    packages = []
    for word in ["langchain", "openai", "pydantic", ...]:
        if word in query.lower():
            packages.append(word)
    
    is_compatibility = "works with" in query or "for" in query
    
    EXAMPLE:
        "PDF parser for langchain"
        -> packages = ["langchain"]
        -> is_compatibility = True

STEP 3: Vector Search (Always)
    vector_results = pinecone_client.search(query, top_k)
    
    This finds repos with similar README content.

STEP 4: Graph Search (If packages detected)
    if packages:
        for package in packages:
            graph_results.extend(
                neo4j_client.find_repos_depending_on(package)
            )
    
    This finds repos that DEPEND ON the mentioned packages.

STEP 5: Fusion (Combine Results)
    all_results = {}
    
    # Add vector results
    for repo in vector_results:
        all_results[repo.full_name] = repo
    
    # Add/boost graph results
    for repo in graph_results:
        if repo.full_name in all_results:
            all_results[repo.full_name].score += 0.5  # BOOST!
        else:
            all_results[repo.full_name] = repo
    
    WHY BOOSTING WORKS:
        If a repo appears in BOTH vector AND graph results,
        it's probably very relevant. We boost its score.

STEP 6: Rank and Return
    final_results = sorted(all_results.values(), 
                          key=lambda x: x.score, 
                          reverse=True)[:top_k]

THE HYBRID ADVANTAGE:
    Query: "PDF parser for langchain"
    
    Vector-only results:
        1. pypdf (mentions PDF a lot)
        2. pdfminer (PDF in name)
        3. unstructured (mentions PDF)
    
    Graph-only results:
        1. langchain (repos depend on it)
        2. llama_index (uses langchain)
    
    HYBRID results:
        1. unstructured (high vector + DEPENDS_ON langchain) <- BEST!
        2. pypdf (high vector score)
        3. langchain-community (graph match)

INTERVIEW TIP:
    "The hybrid approach solves the weaknesses of each individual method. 
    Vector search finds semantic matches but misses structural relationships. 
    Graph search finds dependencies but misses semantic similarity. Together, 
    they provide comprehensive results."


--------------------------------------------------------------------------------
FILE: ingestion/github_fetcher.py
LOCATION: ingestion/github_fetcher.py
PURPOSE: Fetch data from GitHub API
--------------------------------------------------------------------------------

KEY METHODS:

1. fetch_repo(owner, repo) -> dict
   --------------------------------
   API CALL:
       GET https://api.github.com/repos/{owner}/{repo}
   
   RETURNS:
       {
           "full_name": "langchain-ai/langchain",
           "name": "langchain",
           "description": "...",
           "stargazers_count": 75000,
           "language": "Python",
           ...
       }


2. fetch_readme(owner, repo) -> str
   ---------------------------------
   API CALL:
       GET https://api.github.com/repos/{owner}/{repo}/readme
       Accept: application/vnd.github.raw
   
   RETURNS:
       Raw README.md content (first 2000 chars)


3. fetch_dependencies(owner, repo) -> List[str]
   ----------------------------------------------
   API CALL:
       GET https://api.github.com/repos/{owner}/{repo}/contents/requirements.txt
   
   PARSING:
       For each line in requirements.txt:
           - Skip comments (#)
           - Extract package name (before ==, >=, etc)
           - Return list of package names


4. search_repos(query, limit=30) -> List[str]
   -------------------------------------------
   API CALL:
       GET https://api.github.com/search/repositories
           ?q={query}+language:python
           &sort=stars
           &order=desc
   
   RETURNS:
       List of repo full_names sorted by stars

RATE LIMITS:
    - Without token: 60 requests/hour
    - With token: 5000 requests/hour
    
INTERVIEW TIP:
    "I use authenticated GitHub API requests to get 5000 requests per hour 
    instead of 60. The fetcher parses requirements.txt to extract dependencies 
    and construct the dependency graph."


--------------------------------------------------------------------------------
FILE: app/main.py
LOCATION: app/main.py
PURPOSE: Streamlit web interface
--------------------------------------------------------------------------------

STRUCTURE:

1. PAGE CONFIG
    st.set_page_config(
        page_title="GitGraph RAG",
        layout="wide"
    )

2. CUSTOM CSS
    st.markdown("<style>...</style>", unsafe_allow_html=True)
    
    WHY: Streamlit's default styling is basic. Custom CSS makes it look better.

3. HEADER
    st.markdown("GitGraph RAG")
    st.markdown("Hybrid Search Engine...")

4. SEARCH INPUT
    query = st.text_input("Search for repositories")
    top_k = st.slider("Number of results", 1, 10, 5)

5. SEARCH BUTTON
    if st.button("Search"):
        with st.spinner("Searching..."):
            response = search_repos(query, top_k)

6. DISPLAY RESULTS
    for repo in response['results']:
        st.markdown(f"### {repo.name}")
        st.metric("Stars", repo.stars)
        st.metric("Score", repo.score)

7. SIDEBAR
    with st.sidebar:
        st.markdown("## About")
        st.markdown("How it works: ...")

STREAMLIT MAGIC:
    - No HTML/JS/CSS needed (mostly)
    - Pure Python code
    - Auto-refresh on code change
    - Free deployment on Streamlit Cloud

INTERVIEW TIP:
    "I chose Streamlit because it lets me build a web UI in pure Python without 
    learning frontend frameworks. For a portfolio project, this maximizes my 
    time spent on the interesting backend logic."


--------------------------------------------------------------------------------
FILE: ingest_github.py
LOCATION: Root directory
PURPOSE: Populate databases with real GitHub repos
--------------------------------------------------------------------------------

MAIN FLOW:

1. Initialize databases
    pinecone_client.create_index()
    neo4j_client.create_constraints()

2. Search GitHub for repos
    queries = ["langchain", "llm framework", "vector database", "rag"]
    for query in queries:
        repos = github_fetcher.search_repos(query, limit=10)

3. For each repo, ingest it
    def ingest_repo(full_name):
        # Fetch metadata from GitHub
        repo_data = github_fetcher.fetch_repo(owner, repo)
        
        # Fetch README
        readme = github_fetcher.fetch_readme(owner, repo)
        
        # Fetch dependencies
        deps = github_fetcher.fetch_dependencies(owner, repo)
        
        # Add to Pinecone (vector)
        pinecone_client.upsert_repo(full_name, readme, metadata)
        
        # Add to Neo4j (graph)
        neo4j_client.create_repo_node(full_name, metadata)
        
        # Add dependency edges
        for dep in deps:
            neo4j_client.create_dependency(full_name, dep)

RESULT:
    - Pinecone: 45 vectors (repos with embeddings)
    - Neo4j: 157 nodes, 23,236 dependency edges


================================================================================
                        4. DATA FLOW WALKTHROUGH
================================================================================


