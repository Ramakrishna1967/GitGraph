
        full_name: str
        description: str | None = None
        stars: int = 0
        score: float = 0.0
        url: str = ""

HOW IT WORKS:
    1. Pydantic validates data at runtime
    2. If you pass wrong type, it raises an error
    3. Provides automatic serialization (to dict, JSON)

EXAMPLE:
    # This works:
    repo = RepoResult(name="langchain", stars=75000, score=0.95)
    
    # This fails with validation error:
    repo = RepoResult(name="langchain", stars="not a number")

WHY PYDANTIC:
    - Type safety without boilerplate
    - Automatic validation
    - Great IDE support (autocomplete)
    - Easy JSON serialization for APIs

INTERVIEW TIP:
    "I use Pydantic for data validation because it catches type errors at 
    runtime and provides clear error messages. It's especially useful for 
    API responses where you can't trust the data format."


--------------------------------------------------------------------------------
FILE: db/pinecone_client.py
LOCATION: db/pinecone_client.py
PURPOSE: Vector database operations (semantic search)
--------------------------------------------------------------------------------

KEY METHODS:

1. embed_text(text) -> List[float]
   --------------------------------
   PURPOSE: Convert text to a vector (768 numbers)
   
   HOW IT WORKS:
       result = genai.embed_content(
           model="text-embedding-004",
           content=text
       )
       return result['embedding']  # [0.12, -0.45, 0.78, ...]
   
   WHAT'S HAPPENING:
       - Text goes to Gemini's embedding model
       - Model outputs 768 numbers representing the "meaning"
       - Similar texts have similar vectors (close in 768D space)
   
   EXAMPLE:
       embed("async HTTP client") -> [0.1, 0.3, -0.2, ...]
       embed("aiohttp library")   -> [0.12, 0.28, -0.18, ...]
       These vectors are CLOSE because meanings are similar!


2. upsert_repo(repo_id, readme_text, metadata)
   --------------------------------------------
   PURPOSE: Add a repository to the vector database
   
   HOW IT WORKS:
       vector = self.embed_text(readme_text)  # Get vector
       self.index.upsert(vectors=[
           (repo_id, vector, metadata)
       ])
   
   WHAT'S STORED:
       - ID: "langchain-ai/langchain"
       - Vector: [0.12, -0.45, ...] (768 numbers)
       - Metadata: {name, description, stars, url}
   
   NOTE: The full README is NOT stored! Only the vector.


3. search(query, top_k=10) -> List[RepoResult]
   --------------------------------------------
   PURPOSE: Find repos with similar meaning to query
   
   HOW IT WORKS:
       query_vector = self.embed_text(query)  # Convert query to vector
       results = self.index.query(
           vector=query_vector,
           top_k=top_k,
           include_metadata=True
       )
       # Returns repos sorted by similarity score
   
   THE MATH:
       Pinecone uses COSINE SIMILARITY:
       similarity = (A · B) / (||A|| × ||B||)
       
       Range: -1 to 1 (1 = identical, 0 = unrelated, -1 = opposite)

INTERVIEW TIP:
    "Vector search works by converting text to high-dimensional vectors where 
    similar meanings cluster together. Cosine similarity measures the angle 
    between vectors - smaller angle means more similar. This enables semantic 
    search that understands concepts, not just keywords."


--------------------------------------------------------------------------------
FILE: db/neo4j_client.py
LOCATION: db/neo4j_client.py
PURPOSE: Graph database operations (relationship search)
--------------------------------------------------------------------------------

KEY METHODS:

1. create_repo_node(full_name, metadata)
   --------------------------------------
   PURPOSE: Create a node for a repository
   
   CYPHER QUERY:
       MERGE (r:Repository {full_name: $full_name})
       SET r.name = $name,
           r.stars = $stars,
           ...
   
   WHAT THIS CREATES:
       (langchain-ai/langchain)
           - name: "langchain"
           - stars: 75000
           - language: "Python"


2. create_dependency(from_repo, to_repo)
   --------------------------------------
   PURPOSE: Create a DEPENDS_ON relationship
   
   CYPHER QUERY:
       MATCH (from:Repository {full_name: $from_repo})
       MERGE (to:Repository {full_name: $to_repo})
       MERGE (from)-[:DEPENDS_ON]->(to)
   
   WHAT THIS CREATES:
       (langchain) --DEPENDS_ON--> (pydantic)
       (langchain) --DEPENDS_ON--> (openai)


3. find_repos_depending_on(dependency, limit)
   -------------------------------------------
   PURPOSE: Find all repos that use a specific package
   
   CYPHER QUERY:
       MATCH (r:Repository)-[:DEPENDS_ON]->(dep:Repository)
       WHERE dep.name = $dependency
       RETURN r.full_name, r.name, r.stars, ...
       ORDER BY r.stars DESC
   
   EXAMPLE:
       find_repos_depending_on("pydantic") returns:
       - langchain (75000 stars)
       - fastapi (68000 stars)
       - ...

WHY GRAPH DATABASE:
    SQL equivalent would be:
        SELECT r.* FROM repos r
        JOIN dependencies d ON r.id = d.repo_id
        JOIN repos dep ON d.dependency_id = dep.id
        WHERE dep.name = 'pydantic'
    
    This is O(n) for each JOIN!
    
    Neo4j traversal is O(1) per hop - just follow the edge.

INTERVIEW TIP:
    "Graph databases excel at relationship queries. Finding 'repos that depend 
    on X' is a single edge traversal in Neo4j versus expensive JOINs in SQL. 
    For dependency analysis, graph is the natural data model."


--------------------------------------------------------------------------------
FILE: agent/search.py
LOCATION: agent/search.py
PURPOSE: Hybrid search combining vector + graph
--------------------------------------------------------------------------------

MAIN FUNCTION: search_repos(query, top_k=5)

STEP 1: Check Cache
    cache_key = f"{query}_{top_k}"
    if cache_key in _cache:
        return cached_result
    
    WHY: Avoid rate limits and speed up repeated queries

STEP 2: Intent Detection
    packages = []
    for word in ["langchain", "openai", "pydantic", ...]:
        if word in query.lower():
            packages.append(word)
    
    is_compatibility = "works with" in query or "for" in query
    
    EXAMPLE:
        "PDF parser for langchain"
        -> packages = ["langchain"]
        -> is_compatibility = True

STEP 3: Vector Search (Always)
    vector_results = pinecone_client.search(query, top_k)
    
    This finds repos with similar README content.

STEP 4: Graph Search (If packages detected)
    if packages:
        for package in packages:
            graph_results.extend(
                neo4j_client.find_repos_depending_on(package)
            )
    
    This finds repos that DEPEND ON the mentioned packages.

STEP 5: Fusion (Combine Results)
    all_results = {}
    
    # Add vector results
    for repo in vector_results:
        all_results[repo.full_name] = repo
    
    # Add/boost graph results
    for repo in graph_results:
        if repo.full_name in all_results:
            all_results[repo.full_name].score += 0.5  # BOOST!
        else:
            all_results[repo.full_name] = repo
    
    WHY BOOSTING WORKS:
        If a repo appears in BOTH vector AND graph results,
        it's probably very relevant. We boost its score.

STEP 6: Rank and Return
    final_results = sorted(all_results.values(), 
                          key=lambda x: x.score, 
                          reverse=True)[:top_k]

THE HYBRID ADVANTAGE:
    Query: "PDF parser for langchain"
    
    Vector-only results:
        1. pypdf (mentions PDF a lot)
        2. pdfminer (PDF in name)
        3. unstructured (mentions PDF)
    
    Graph-only results:
        1. langchain (repos depend on it)
        2. llama_index (uses langchain)
    
    HYBRID results:
        1. unstructured (high vector + DEPENDS_ON langchain) <- BEST!
        2. pypdf (high vector score)
        3. langchain-community (graph match)

INTERVIEW TIP:
    "The hybrid approach solves the weaknesses of each individual method. 
    Vector search finds semantic matches but misses structural relationships. 
    Graph search finds dependencies but misses semantic similarity. Together, 
    they provide comprehensive results."


--------------------------------------------------------------------------------
FILE: ingestion/github_fetcher.py
LOCATION: ingestion/github_fetcher.py
PURPOSE: Fetch data from GitHub API
--------------------------------------------------------------------------------

KEY METHODS:

1. fetch_repo(owner, repo) -> dict
   --------------------------------
   API CALL:
       GET https://api.github.com/repos/{owner}/{repo}
   
   RETURNS:
       {
           "full_name": "langchain-ai/langchain",
           "name": "langchain",
           "description": "...",
           "stargazers_count": 75000,
           "language": "Python",
           ...
       }


2. fetch_readme(owner, repo) -> str
   ---------------------------------
   API CALL:
       GET https://api.github.com/repos/{owner}/{repo}/readme
       Accept: application/vnd.github.raw
   
   RETURNS:
       Raw README.md content (first 2000 chars)


3. fetch_dependencies(owner, repo) -> List[str]
   ----------------------------------------------
   API CALL:
       GET https://api.github.com/repos/{owner}/{repo}/contents/requirements.txt
   
   PARSING:
       For each line in requirements.txt:
           - Skip comments (#)
           - Extract package name (before ==, >=, etc)
           - Return list of package names


4. search_repos(query, limit=30) -> List[str]
   -------------------------------------------
   API CALL:
       GET https://api.github.com/search/repositories
           ?q={query}+language:python
           &sort=stars
           &order=desc
   
   RETURNS:
       List of repo full_names sorted by stars

RATE LIMITS:
    - Without token: 60 requests/hour
    - With token: 5000 requests/hour
    
INTERVIEW TIP:
    "I use authenticated GitHub API requests to get 5000 requests per hour 
    instead of 60. The fetcher parses requirements.txt to extract dependencies 
    and construct the dependency graph."


--------------------------------------------------------------------------------
FILE: app/main.py
LOCATION: app/main.py
PURPOSE: Streamlit web interface
--------------------------------------------------------------------------------

STRUCTURE:

1. PAGE CONFIG
    st.set_page_config(
        page_title="GitGraph RAG",
        layout="wide"
    )

2. CUSTOM CSS
    st.markdown("<style>...</style>", unsafe_allow_html=True)
    
    WHY: Streamlit's default styling is basic. Custom CSS makes it look better.

3. HEADER
    st.markdown("GitGraph RAG")
    st.markdown("Hybrid Search Engine...")

4. SEARCH INPUT
    query = st.text_input("Search for repositories")
    top_k = st.slider("Number of results", 1, 10, 5)

5. SEARCH BUTTON
    if st.button("Search"):
        with st.spinner("Searching..."):
            response = search_repos(query, top_k)

6. DISPLAY RESULTS
    for repo in response['results']:
        st.markdown(f"### {repo.name}")
        st.metric("Stars", repo.stars)
        st.metric("Score", repo.score)

7. SIDEBAR
    with st.sidebar:
        st.markdown("## About")
        st.markdown("How it works: ...")

STREAMLIT MAGIC:
    - No HTML/JS/CSS needed (mostly)
    - Pure Python code
    - Auto-refresh on code change
    - Free deployment on Streamlit Cloud

INTERVIEW TIP:
    "I chose Streamlit because it lets me build a web UI in pure Python without 
    learning frontend frameworks. For a portfolio project, this maximizes my 
    time spent on the interesting backend logic."


--------------------------------------------------------------------------------
FILE: ingest_github.py
LOCATION: Root directory
PURPOSE: Populate databases with real GitHub repos
--------------------------------------------------------------------------------

MAIN FLOW:

1. Initialize databases
    pinecone_client.create_index()
    neo4j_client.create_constraints()

2. Search GitHub for repos
    queries = ["langchain", "llm framework", "vector database", "rag"]
    for query in queries:
        repos = github_fetcher.search_repos(query, limit=10)

3. For each repo, ingest it
    def ingest_repo(full_name):
        # Fetch metadata from GitHub
        repo_data = github_fetcher.fetch_repo(owner, repo)
        
        # Fetch README
        readme = github_fetcher.fetch_readme(owner, repo)
        
        # Fetch dependencies
        deps = github_fetcher.fetch_dependencies(owner, repo)
        
        # Add to Pinecone (vector)
        pinecone_client.upsert_repo(full_name, readme, metadata)
        
        # Add to Neo4j (graph)
        neo4j_client.create_repo_node(full_name, metadata)
        
        # Add dependency edges
        for dep in deps:
            neo4j_client.create_dependency(full_name, dep)

RESULT:
    - Pinecone: 45 vectors (repos with embeddings)
    - Neo4j: 157 nodes, 23,236 dependency edges


================================================================================
                        4. DATA FLOW WALKTHROUGH
================================================================================


