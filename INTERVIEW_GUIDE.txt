

================================================================================
                          2. ARCHITECTURE DIAGRAM
================================================================================

USER QUERY: "PDF parser for langchain"
                    |
                    v
            +---------------+
            |   Streamlit   |  <-- Frontend (app/main.py)
            |   Web UI      |
            +---------------+
                    |
                    v
            +---------------+
            |  search.py    |  <-- Search Logic (agent/search.py)
            |  (Hybrid)     |
            +-------+-------+
                    |
        +-----------+-----------+
        |                       |
        v                       v
+---------------+       +---------------+
|   Pinecone    |       |    Neo4j      |
|   (Vectors)   |       |   (Graph)     |
+---------------+       +---------------+
        |                       |
        v                       v
  "Find repos           "Find repos that
   with similar          DEPEND ON
   README meaning"       langchain"
        |                       |
        +-----------+-----------+
                    |
                    v
            +---------------+
            |    FUSION     |  <-- Combine & Rank Results
            +---------------+
                    |
                    v
            +---------------+
            |   RESULTS     |  <-- Display to User
            +---------------+


================================================================================
                      3. FILE-BY-FILE EXPLANATION
================================================================================

--------------------------------------------------------------------------------
FILE: .env
LOCATION: Root directory
PURPOSE: Store secret API keys
--------------------------------------------------------------------------------

CONTENTS:
    GOOGLE_API_KEY=AIza...      # Gemini API for embeddings
    PINECONE_API_KEY=pcsk...    # Pinecone vector database
    NEO4J_URI=neo4j+s://...     # Neo4j connection string
    NEO4J_PASSWORD=...          # Neo4j authentication
    GITHUB_TOKEN=ghp_...        # GitHub API access

WHY THIS FILE EXISTS:
    - NEVER hardcode secrets in source code
    - .gitignore prevents this file from being committed
    - Each environment (dev, prod) can have different values

INTERVIEW TIP:
    "I use environment variables for secrets so they're never in version 
    control. This follows the 12-factor app methodology."


--------------------------------------------------------------------------------
FILE: config/settings.py
LOCATION: config/settings.py
PURPOSE: Centralized configuration management
--------------------------------------------------------------------------------

CODE:
    import os
    from dotenv import load_dotenv
    
    load_dotenv()  # Reads .env file
    
    class Settings:
        GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
        PINECONE_API_KEY = os.getenv("PINECONE_API_KEY", "")
        NEO4J_URI = os.getenv("NEO4J_URI", "")
        ...
    
    settings = Settings()  # Single instance

HOW IT WORKS:
    1. load_dotenv() reads the .env file
    2. os.getenv() fetches each variable
    3. Settings class holds all config in one place
    4. Other files import: from config import settings

WHY THIS PATTERN:
    - Single source of truth for config
    - Easy to add validation
    - Easy to mock in tests
    - Type hints for IDE support

INTERVIEW TIP:
    "Centralized config prevents scattered os.getenv calls throughout the 
    codebase and makes configuration changes easier to manage."


--------------------------------------------------------------------------------
FILE: db/schemas.py
LOCATION: db/schemas.py  
PURPOSE: Data validation and type safety using Pydantic
--------------------------------------------------------------------------------

CODE:
    from pydantic import BaseModel
    
    class RepoMetadata(BaseModel):
        name: str
        full_name: str
        description: str | None = None
        stars: int = 0
        language: str | None = None
        url: str = ""
    
    class RepoResult(BaseModel):
        name: str
        full_name: str
        description: str | None = None
        stars: int = 0
        score: float = 0.0
        url: str = ""

HOW IT WORKS:
    1. Pydantic validates data at runtime
    2. If you pass wrong type, it raises an error
    3. Provides automatic serialization (to dict, JSON)

EXAMPLE:
    # This works:
    repo = RepoResult(name="langchain", stars=75000, score=0.95)
    
    # This fails with validation error:
    repo = RepoResult(name="langchain", stars="not a number")

WHY PYDANTIC:
    - Type safety without boilerplate
    - Automatic validation
    - Great IDE support (autocomplete)
    - Easy JSON serialization for APIs

INTERVIEW TIP:
    "I use Pydantic for data validation because it catches type errors at 
    runtime and provides clear error messages. It's especially useful for 
    API responses where you can't trust the data format."


--------------------------------------------------------------------------------
FILE: db/pinecone_client.py
LOCATION: db/pinecone_client.py
PURPOSE: Vector database operations (semantic search)
--------------------------------------------------------------------------------

KEY METHODS:

1. embed_text(text) -> List[float]
   --------------------------------
   PURPOSE: Convert text to a vector (768 numbers)
   
   HOW IT WORKS:
       result = genai.embed_content(
           model="text-embedding-004",
           content=text
       )
       return result['embedding']  # [0.12, -0.45, 0.78, ...]
   
   WHAT'S HAPPENING:
       - Text goes to Gemini's embedding model
       - Model outputs 768 numbers representing the "meaning"
       - Similar texts have similar vectors (close in 768D space)
   
   EXAMPLE:
       embed("async HTTP client") -> [0.1, 0.3, -0.2, ...]
       embed("aiohttp library")   -> [0.12, 0.28, -0.18, ...]
       These vectors are CLOSE because meanings are similar!


2. upsert_repo(repo_id, readme_text, metadata)
   --------------------------------------------
   PURPOSE: Add a repository to the vector database
   
   HOW IT WORKS:
       vector = self.embed_text(readme_text)  # Get vector
       self.index.upsert(vectors=[
           (repo_id, vector, metadata)
       ])
   
   WHAT'S STORED:
       - ID: "langchain-ai/langchain"
       - Vector: [0.12, -0.45, ...] (768 numbers)
       - Metadata: {name, description, stars, url}
   
   NOTE: The full README is NOT stored! Only the vector.


3. search(query, top_k=10) -> List[RepoResult]
   --------------------------------------------
   PURPOSE: Find repos with similar meaning to query
   
   HOW IT WORKS:
       query_vector = self.embed_text(query)  # Convert query to vector
       results = self.index.query(
           vector=query_vector,
           top_k=top_k,
           include_metadata=True
       )
       # Returns repos sorted by similarity score
   
   THE MATH:
       Pinecone uses COSINE SIMILARITY:
       similarity = (A · B) / (||A|| × ||B||)
       
       Range: -1 to 1 (1 = identical, 0 = unrelated, -1 = opposite)

INTERVIEW TIP:
    "Vector search works by converting text to high-dimensional vectors where 
    similar meanings cluster together. Cosine similarity measures the angle 
    between vectors - smaller angle means more similar. This enables semantic 
    search that understands concepts, not just keywords."


--------------------------------------------------------------------------------
FILE: db/neo4j_client.py
LOCATION: db/neo4j_client.py
PURPOSE: Graph database operations (relationship search)
--------------------------------------------------------------------------------

KEY METHODS:

1. create_repo_node(full_name, metadata)
   --------------------------------------
   PURPOSE: Create a node for a repository
   
   CYPHER QUERY:
       MERGE (r:Repository {full_name: $full_name})
       SET r.name = $name,
           r.stars = $stars,
           ...
   
   WHAT THIS CREATES:
       (langchain-ai/langchain)
           - name: "langchain"
           - stars: 75000
           - language: "Python"


2. create_dependency(from_repo, to_repo)
   --------------------------------------
   PURPOSE: Create a DEPENDS_ON relationship
   
   CYPHER QUERY:
       MATCH (from:Repository {full_name: $from_repo})
       MERGE (to:Repository {full_name: $to_repo})
       MERGE (from)-[:DEPENDS_ON]->(to)
   
   WHAT THIS CREATES:
       (langchain) --DEPENDS_ON--> (pydantic)
       (langchain) --DEPENDS_ON--> (openai)


3. find_repos_depending_on(dependency, limit)
   -------------------------------------------
   PURPOSE: Find all repos that use a specific package
   
   CYPHER QUERY:
       MATCH (r:Repository)-[:DEPENDS_ON]->(dep:Repository)
       WHERE dep.name = $dependency
       RETURN r.full_name, r.name, r.stars, ...
       ORDER BY r.stars DESC
   
   EXAMPLE:
       find_repos_depending_on("pydantic") returns:
       - langchain (75000 stars)
       - fastapi (68000 stars)
       - ...

WHY GRAPH DATABASE:
    SQL equivalent would be:
        SELECT r.* FROM repos r
        JOIN dependencies d ON r.id = d.repo_id
        JOIN repos dep ON d.dependency_id = dep.id
        WHERE dep.name = 'pydantic'
    
    This is O(n) for each JOIN!
    
    Neo4j traversal is O(1) per hop - just follow the edge.

INTERVIEW TIP:
    "Graph databases excel at relationship queries. Finding 'repos that depend 
    on X' is a single edge traversal in Neo4j versus expensive JOINs in SQL. 
    For dependency analysis, graph is the natural data model."


--------------------------------------------------------------------------------
FILE: agent/search.py
LOCATION: agent/search.py
PURPOSE: Hybrid search combining vector + graph
--------------------------------------------------------------------------------

MAIN FUNCTION: search_repos(query, top_k=5)

STEP 1: Check Cache
    cache_key = f"{query}_{top_k}"
    if cache_key in _cache:
        return cached_result
    
    WHY: Avoid rate limits and speed up repeated queries

STEP 2: Intent Detection
    packages = []
    for word in ["langchain", "openai", "pydantic", ...]:
        if word in query.lower():
            packages.append(word)
    
    is_compatibility = "works with" in query or "for" in query
    
    EXAMPLE:
        "PDF parser for langchain"
        -> packages = ["langchain"]
        -> is_compatibility = True

STEP 3: Vector Search (Always)
    vector_results = pinecone_client.search(query, top_k)
    
    This finds repos with similar README content.

STEP 4: Graph Search (If packages detected)
    if packages:
        for package in packages:
            graph_results.extend(
                neo4j_client.find_repos_depending_on(package)
            )
    
    This finds repos that DEPEND ON the mentioned packages.

STEP 5: Fusion (Combine Results)
    all_results = {}
    
    # Add vector results
    for repo in vector_results:
        all_results[repo.full_name] = repo
    
    # Add/boost graph results
    for repo in graph_results:
        if repo.full_name in all_results:
            all_results[repo.full_name].score += 0.5  # BOOST!
        else:
            all_results[repo.full_name] = repo
    
    WHY BOOSTING WORKS:
        If a repo appears in BOTH vector AND graph results,
        it's probably very relevant. We boost its score.

STEP 6: Rank and Return
    final_results = sorted(all_results.values(), 
                          key=lambda x: x.score, 
                          reverse=True)[:top_k]

THE HYBRID ADVANTAGE:
    Query: "PDF parser for langchain"
    
    Vector-only results:
        1. pypdf (mentions PDF a lot)
        2. pdfminer (PDF in name)
        3. unstructured (mentions PDF)
    
    Graph-only results:
        1. langchain (repos depend on it)
        2. llama_index (uses langchain)
    
    HYBRID results:
        1. unstructured (high vector + DEPENDS_ON langchain) <- BEST!
        2. pypdf (high vector score)
        3. langchain-community (graph match)

INTERVIEW TIP:
    "The hybrid approach solves the weaknesses of each individual method. 
    Vector search finds semantic matches but misses structural relationships. 
    Graph search finds dependencies but misses semantic similarity. Together, 
    they provide comprehensive results."


--------------------------------------------------------------------------------
FILE: ingestion/github_fetcher.py
LOCATION: ingestion/github_fetcher.py
PURPOSE: Fetch data from GitHub API
--------------------------------------------------------------------------------

KEY METHODS:

1. fetch_repo(owner, repo) -> dict
   --------------------------------
   API CALL:
       GET https://api.github.com/repos/{owner}/{repo}
   
   RETURNS:
       {
           "full_name": "langchain-ai/langchain",
           "name": "langchain",
           "description": "...",
           "stargazers_count": 75000,
           "language": "Python",
           ...
       }


2. fetch_readme(owner, repo) -> str
   ---------------------------------
   API CALL:
       GET https://api.github.com/repos/{owner}/{repo}/readme
       Accept: application/vnd.github.raw
   
   RETURNS:
       Raw README.md content (first 2000 chars)


3. fetch_dependencies(owner, repo) -> List[str]
   ----------------------------------------------
   API CALL:
       GET https://api.github.com/repos/{owner}/{repo}/contents/requirements.txt
   
   PARSING:
       For each line in requirements.txt:
           - Skip comments (#)
           - Extract package name (before ==, >=, etc)
           - Return list of package names


4. search_repos(query, limit=30) -> List[str]
   -------------------------------------------
   API CALL:
       GET https://api.github.com/search/repositories
           ?q={query}+language:python
           &sort=stars
           &order=desc
   
   RETURNS:
       List of repo full_names sorted by stars

RATE LIMITS:
    - Without token: 60 requests/hour
    - With token: 5000 requests/hour
    
INTERVIEW TIP:
    "I use authenticated GitHub API requests to get 5000 requests per hour 
    instead of 60. The fetcher parses requirements.txt to extract dependencies 
    and construct the dependency graph."


--------------------------------------------------------------------------------
FILE: app/main.py
LOCATION: app/main.py
PURPOSE: Streamlit web interface
--------------------------------------------------------------------------------

STRUCTURE:

1. PAGE CONFIG
    st.set_page_config(
        page_title="GitGraph RAG",
        layout="wide"
    )

2. CUSTOM CSS
    st.markdown("<style>...</style>", unsafe_allow_html=True)
    
    WHY: Streamlit's default styling is basic. Custom CSS makes it look better.

3. HEADER
    st.markdown("GitGraph RAG")
    st.markdown("Hybrid Search Engine...")

4. SEARCH INPUT
    query = st.text_input("Search for repositories")
    top_k = st.slider("Number of results", 1, 10, 5)

5. SEARCH BUTTON
    if st.button("Search"):
        with st.spinner("Searching..."):
            response = search_repos(query, top_k)

6. DISPLAY RESULTS
    for repo in response['results']:
        st.markdown(f"### {repo.name}")
        st.metric("Stars", repo.stars)
        st.metric("Score", repo.score)

7. SIDEBAR
    with st.sidebar:
        st.markdown("## About")
        st.markdown("How it works: ...")

STREAMLIT MAGIC:
    - No HTML/JS/CSS needed (mostly)
    - Pure Python code
    - Auto-refresh on code change
    - Free deployment on Streamlit Cloud

INTERVIEW TIP:
    "I chose Streamlit because it lets me build a web UI in pure Python without 
    learning frontend frameworks. For a portfolio project, this maximizes my 
    time spent on the interesting backend logic."


--------------------------------------------------------------------------------
FILE: ingest_github.py
LOCATION: Root directory
PURPOSE: Populate databases with real GitHub repos
--------------------------------------------------------------------------------

MAIN FLOW:

1. Initialize databases
    pinecone_client.create_index()
    neo4j_client.create_constraints()

2. Search GitHub for repos
    queries = ["langchain", "llm framework", "vector database", "rag"]
    for query in queries:
        repos = github_fetcher.search_repos(query, limit=10)

3. For each repo, ingest it
    def ingest_repo(full_name):
        # Fetch metadata from GitHub
        repo_data = github_fetcher.fetch_repo(owner, repo)
        
        # Fetch README
        readme = github_fetcher.fetch_readme(owner, repo)
        
        # Fetch dependencies
        deps = github_fetcher.fetch_dependencies(owner, repo)
        
        # Add to Pinecone (vector)
        pinecone_client.upsert_repo(full_name, readme, metadata)
        
        # Add to Neo4j (graph)
        neo4j_client.create_repo_node(full_name, metadata)
        
        # Add dependency edges
        for dep in deps:
            neo4j_client.create_dependency(full_name, dep)

RESULT:
    - Pinecone: 45 vectors (repos with embeddings)
    - Neo4j: 157 nodes, 23,236 dependency edges


================================================================================
                        4. DATA FLOW WALKTHROUGH
================================================================================

Let's trace a query: "PDF parser for langchain"

STEP 1: User types query in Streamlit
    app/main.py:
        query = "PDF parser for langchain"
        response = search_repos(query, top_k=5)

STEP 2: search.py receives the query
    agent/search.py:
        # Check cache - miss
        # Detect intent
        packages = ["langchain"]  # Found "langchain" in query
        is_compatibility = True   # "for" keyword detected

STEP 3: Vector search in Pinecone
    pinecone_client.search("PDF parser for langchain")
    
    1. Convert query to vector using Gemini
    2. Find 5 closest vectors in Pinecone
    3. Return: [unstructured(0.85), pypdf(0.82), langchain(0.78), ...]

STEP 4: Graph search in Neo4j
    neo4j_client.find_repos_depending_on("langchain")
    
    1. Run Cypher query
    2. Traverse DEPENDS_ON edges
    3. Return: [unstructured, llama_index, chroma, ...]

STEP 5: Fusion
    all_results = {
        "unstructured": score 0.85 + 0.5 = 1.35  # BOOSTED!
        "pypdf": score 0.82
        "langchain": score 0.78
        "llama_index": score 1.0 (graph only)
        ...
    }

STEP 6: Rank and return
    final = [
        unstructured (1.35),  # Best because found in BOTH
        llama_index (1.0),
        pypdf (0.82),
        langchain (0.78),
        ...
    ]

STEP 7: Display in Streamlit
    For each result:
        Show name, stars, score, description


================================================================================
                   5. KEY INTERVIEW QUESTIONS & ANSWERS
================================================================================

Q1: "Why did you use a vector database instead of just keyword search?"
A1: "Keyword search fails when users don't know the exact terminology. If 
    someone searches 'async HTTP client', keyword search misses 'aiohttp' and 
    'httpx'. Vector search captures semantic meaning, so conceptually similar 
    repos match even without exact keywords."

Q2: "Why Neo4j instead of storing relationships in PostgreSQL?"
A2: "Graph databases are optimized for relationship traversal. Finding 'repos 
    that depend on X' is a single hop in Neo4j. In SQL, this requires JOINs 
    which are O(n) per operation. For dependency graphs with deep chains, 
    Neo4j is orders of magnitude faster."

Q3: "How do embeddings work?"
A3: "The embedding model converts text into a high-dimensional vector (768 
    numbers). Similar texts have similar vectors because the model learned 
    semantic relationships during training. We use cosine similarity to 
    measure how close two vectors are - closer means more similar meaning."

Q4: "What's the hybrid approach advantage?"
A4: "Vector search finds semantic matches but doesn't know about dependencies. 
    Graph search knows relationships but can't understand meaning. The hybrid 
    approach combines both: if a repo matches semantically AND has the right 
    dependencies, it gets boosted. This gives more relevant results than 
    either approach alone."

Q5: "How did you handle rate limits?"
A5: "Multiple strategies: (1) Caching - store results for 5 minutes to avoid 
    repeated API calls. (2) Authenticated GitHub requests - 5000/hour vs 60. 
    (3) Simplified search - removed Gemini API calls for intent detection 
    to stay within free tier limits."

Q6: "Why these specific technologies?"
A6: "All free tier: Pinecone gives 100K vectors free, Neo4j Aura gives 200K 
    nodes free, Gemini gives 15 RPM free. Streamlit has free hosting. This 
    proves I can architect systems within constraints - important for 
    startups with limited budgets."

Q7: "How would you scale this?"
A7: "Pinecone is already serverless and scales automatically. Neo4j Aura 
    has paid tiers with more capacity. For higher load, I'd add: (1) Redis 
    for caching, (2) Background job queue for ingestion, (3) CDN for static 
    assets. The architecture is already horizontally scalable."

Q8: "What would you improve with more time?"
A8: "Three things: (1) Add ALTERNATIVE_TO relationships in the graph - repos 
    that compete with each other. (2) Implement user 'Stack DNA' profiles - 
    track which repos a user has starred to personalize recommendations. 
    (3) Add automated daily ingestion via GitHub Actions."


================================================================================
                    6. TECHNOLOGY CHOICES & JUSTIFICATIONS
================================================================================

PINECONE (Vector Database)
--------------------------
Chose over: Weaviate, Milvus, Qdrant
Why: Serverless (no infrastructure), free tier (100K vectors), fast

ALTERNATIVES I CONSIDERED:
    - Weaviate: Requires self-hosting or paid cloud
    - Milvus: Complex setup, better for on-premise
    - Qdrant: Good but Pinecone has better free tier
    - Chroma: In-memory only, not for production


NEO4J (Graph Database)
----------------------
Chose over: Amazon Neptune, TigerGraph, DGraph
Why: Best free tier (200K nodes), great Cypher query language, mature

ALTERNATIVES I CONSIDERED:
    - Neptune: AWS only, expensive
    - TigerGraph: Complex licensing
    - DGraph: Less mature, smaller community
    - NetworkX: Python library, not a database (no persistence)


GEMINI (LLM & Embeddings)
-------------------------
Chose over: OpenAI, Cohere, Voyage
Why: Generous free tier (15 RPM), 1M token context, good embeddings

ALTERNATIVES I CONSIDERED:
    - OpenAI: Expensive, pay-per-token
    - Cohere: Good but smaller free tier
    - Voyage: Great embeddings but less known


STREAMLIT (Frontend)
--------------------
Chose over: React, Next.js, Gradio
Why: Pure Python, fast development, free hosting

ALTERNATIVES I CONSIDERED:
    - React: Overkill for a demo, requires JS knowledge
    - Gradio: Good but Streamlit is more flexible
    - Flask: Need to build UI from scratch


================================================================================
                              CONCLUSION
================================================================================

GitGraph RAG demonstrates:
    1. Vector search for semantic understanding
    2. Graph traversal for structural relationships
    3. Hybrid fusion for better results
    4. Production-ready architecture on $0 budget

The code is clean, typed, and well-structured. Each component has a single 
responsibility. The system is horizontally scalable and uses proven 
technologies.

This project shows I can:
    - Design complex systems
    - Work within constraints (free tier)
    - Make justified technology decisions
    - Write clean, maintainable code
    - Explain technical concepts clearly

================================================================================
                            END OF DOCUMENTATION
================================================================================
